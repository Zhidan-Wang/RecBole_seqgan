import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
from torch.nn.init import xavier_normal_, constant_
from recbole.model.abstract_recommender import SequentialRecommender
from recbole.model.loss import BPRLoss

class Discriminator(SequentialRecommender):

    def __init__(self, config, dataset):
        super(Discriminator, self).__init__(config, dataset)
        self.emb_dim = config['embedding_size']
        self.hidden_dim = config['hidden_size']
        self.device = config['device']
        self.item_embedding = nn.Embedding(self.n_items, self.emb_dim, padding_idx=0)
        self.gru = nn.GRU(self.emb_dim, self.hidden_dim, batch_first=True)
        self.lin = nn.Linear(self.hidden_dim, 2)
        self.loss_type = config['loss_type']
        if self.loss_type == 'BPR':
            self.loss_fct = BPRLoss()
        else:
            if self.loss_type == 'CE':
                self.loss_fct = nn.CrossEntropyLoss()
            else:
                raise NotImplementedError("Make sure 'loss_type' in ['BPR', 'CE']!")
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight.data)
        else:
            if isinstance(module, nn.Linear):
                xavier_normal_(module.weight.data)
                if module.bias is not None:
                    constant_(module.bias.data, 0)

    def forward(self, item_seq):
        """
        Args:
            item_seq: (batch_size, seq_len), sequence of tokens generated by generator
        """
        emb = self.item_embedding(item_seq)
        h0 = self.init_hidden(item_seq.size(0))
        output, h = self.gru(emb, h0)
        seq_output = self.lin(h.contiguous().view(-1, self.hidden_dim))
        return seq_output

    def init_hidden(self, batch_size):
        h = Variable(torch.zeros((1, batch_size, self.hidden_dim)))
        return h

    def calculate_loss(self, interaction):
        item_seq = interaction[self.ITEM_SEQ]
        seq_output = self.forward(item_seq)
        pos_items = interaction['label']

        if self.loss_type == 'BPR':
            neg_items = interaction[self.NEG_ITEM_ID]
            pos_items_emb = self.item_embedding(pos_items)
            neg_items_emb = self.item_embedding(neg_items)
            pos_score = torch.sum((seq_output * pos_items_emb), dim=(-1))
            neg_score = torch.sum((seq_output * neg_items_emb), dim=(-1))
            loss = self.loss_fct(pos_score, neg_score)
            return loss
        logits = seq_output
        loss = self.loss_fct(logits, pos_items)
        return loss

    def predict(self, interaction):
        item_seq = interaction[self.ITEM_SEQ]
        item_seq_len = interaction[self.ITEM_SEQ_LEN]
        labels = interaction['label']

        seq_output = self.forward(item_seq)
        softmax = nn.Softmax(dim=1)
        pred = softmax(seq_output)
        preds = pred.argmax(axis = 1)

        n_hits = (labels == preds).nonzero(as_tuple=False)[:, :-1].size(0)
        #print("hits###############", n_hits)
        scores = n_hits

        return scores
