import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
from torch.nn.init import xavier_normal_, constant_
from recbole.model.abstract_recommender import SequentialRecommender
from recbole.model.loss import BPRLoss

class Generator(SequentialRecommender):

    def __init__(self, config, dataset):
        super(Generator, self).__init__(config, dataset)
        self.emb_dim = config['embedding_size']
        self.hidden_dim = config['hidden_size']
        self.device = config['device']
        self.item_embedding = nn.Embedding(self.n_items, self.emb_dim, padding_idx=0)
        self.gru = nn.GRU(self.emb_dim, self.hidden_dim, batch_first=True)
        self.lin = nn.Linear(self.hidden_dim, self.emb_dim)
        self.softmax = nn.LogSoftmax(dim=1)
        self.loss_type = config['loss_type']
        if self.loss_type == 'BPR':
            self.loss_fct = BPRLoss()
        else:
            if self.loss_type == 'CE':
                self.loss_fct = nn.CrossEntropyLoss()
            else:
                raise NotImplementedError("Make sure 'loss_type' in ['BPR', 'CE']!")
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight.data)
        else:
            if isinstance(module, nn.Linear):
                xavier_normal_(module.weight.data)
                if module.bias is not None:
                    constant_(module.bias.data, 0)

    def forward(self, item_seq):
        """
        Args:
            item_seq: (batch_size, seq_len), sequence of tokens generated by generator
        """
        emb = self.item_embedding(item_seq)
        h0 = self.init_hidden(item_seq.size(0))
        output, h = self.gru(emb, h0)
        seq_output = self.lin(h.contiguous().view(-1, self.hidden_dim))
        return seq_output

    def calculate_loss(self, interaction):
        item_seq = interaction[self.ITEM_SEQ]
        seq_output = self.forward(item_seq)
        pos_items = interaction[self.POS_ITEM_ID]
        if self.loss_type == 'BPR':
            neg_items = interaction[self.NEG_ITEM_ID]
            pos_items_emb = self.item_embedding(pos_items)
            neg_items_emb = self.item_embedding(neg_items)
            pos_score = torch.sum((seq_output * pos_items_emb), dim=(-1))
            neg_score = torch.sum((seq_output * neg_items_emb), dim=(-1))
            loss = self.loss_fct(pos_score, neg_score)
            return loss
        test_item_emb = self.item_embedding.weight
        logits = torch.matmul(seq_output, test_item_emb.transpose(0, 1))
        loss = self.loss_fct(logits, pos_items)
        return loss

    def predict(self, interaction):
        item_seq = interaction[self.ITEM_SEQ]
        item_seq_len = interaction[self.ITEM_SEQ_LEN]
        test_item = interaction[self.ITEM_ID]
        seq_output = self.forward(item_seq)
        test_item_emb = self.item_embedding(test_item)
        scores = torch.mul(seq_output, test_item_emb).sum(dim=1)
        return scores

    def full_sort_predict(self, interaction):
        item_seq = interaction[self.ITEM_SEQ]
        item_seq_len = interaction[self.ITEM_SEQ_LEN]
        seq_output = self.forward(item_seq)
        test_items_emb = self.item_embedding.weight
        scores = torch.matmul(seq_output, test_items_emb.transpose(0, 1))
        return scores

    def step(self, x, h):
        """
        Args:
            x: (batch_size,  1), sequence of tokens generated by generator
            h: (1, batch_size, hidden_dim), lstm hidden state
        """
        emb = self.item_embedding(x)
        output, h = self.gru(emb, h)
        pred = self.lin(h.view(-1, self.hidden_dim))
        return pred, h

    def init_hidden(self, batch_size):
        h = Variable(torch.zeros((1, batch_size, self.hidden_dim)))
        return h

    def init_params(self):
        for param in self.parameters():
            param.data.uniform_(-0.05, 0.05)

    def sample11(self, batch_size, seq_len, x=None):
        res = []
        flag = False
        if x is None:
            flag = True
        if flag:
            x = Variable(torch.zeros((batch_size, 1)).long())
        x = x.to(self.device)
        h = self.init_hidden(batch_size)
        samples = []
        if flag:
            for i in range(seq_len):
                output, h = self.step(x, h)
            x = output.multinomial(1)
            samples.append(x)

        else:
            given_len = x.size(1)
            lis = x.chunk((x.size(1)), dim=1)
            for i in range(given_len):
                output, h = self.step(lis[i], h)
                samples.append(lis[i])
            else:
                x = output.multinomial(1)
                for i in range(given_len, seq_len):
                    samples.append(x)
                    output, h = self.step(x, h)
                    x = output.multinomial(1)
        output = torch.cat(samples, dim=1)

        return output

    def sample(self, batch_size, seq_len, x=None):
        res = []
        flag = False # whether sample from zero
        if x is None:
            flag = True
        if flag:
            x = Variable(torch.zeros((batch_size, 1)).long())
        x = x.to(self.device)
        h = self.init_hidden(batch_size)
        samples = []
        if flag:
            for i in range(seq_len):
                output, h = self.step(x, h)

                item_emb = self.item_embedding.weight
                pred = torch.matmul(output, item_emb.transpose(0, 1))
                pred = F.softmax(pred, dim=1)
                x = pred.multinomial(1)
                samples.append(x)
        else:
            given_len = x.size(1)
            lis = x.chunk(x.size(1), dim=1)
            for i in range(given_len):
                output, h = self.step(lis[i], h)
                samples.append(lis[i])

            item_emb = self.item_embedding.weight
            pred = torch.matmul(output, item_emb.transpose(0, 1))
            pred = F.softmax(pred, dim=1)
            x = pred.multinomial(1)

            for i in range(given_len, seq_len):
                samples.append(x)
                output, h = self.step(x, h)

                item_emb = self.item_embedding.weight
                pred = torch.matmul(output, item_emb.transpose(0, 1))
                pred = F.softmax(pred, dim=1)
                x = pred.multinomial(1)

        output = torch.cat(samples, dim=1)
        return output